import asyncio
import csv
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import json
import os

# Timeouts (ms) y waits
GOTO_TIMEOUT_START = 15000       # 15s para la home
GOTO_TIMEOUT_CATEGORY = 15000    # 15s para p√°ginas de categor√≠a
SHORT_WAIT = 500                 # 0.5s para wait_for_timeout
CLICK_WAIT = 500                 # 0.5s tras click


SITES = {
    "biobiochile": {
        "start_url": "https://www.biobiochile.cl/",     # URL de la p√°gina
        "category_pattern": "lista/categorias",         # Slug p√°gina para reconcoer categorias
        "news_pattern": ["/noticias/"],                 # Slug p√°gina para reconocer links de noticias
        "load_more_selector": ".fetch-btn",             # Classname boton cargar mas links de la p√°gina
        "pagination_type": "loadmore",                  # Forma en que se cargan mas links, loadmore asume boton jscript
        "max_clicks": 2                                 # Cantidad m√°xima de clikcs de este boton en la p√°gina
    }
}

# Funci√≥n para bloquear recursos al buscar links de noticias desde una p√°gina con bot√≥n "Cargar m√°s"
async def _block_assets(page):
    async def handler(route):
        req = route.request
        if req.resource_type in ("image", "stylesheet", "font", "media"):
            await route.abort()
        else:
            await route.continue_()
    await page.route("**/*", handler)

async def scrape_category_loadmore(page, category_url, load_more_selector, news_pattern, max_clicks=10):
    news_links = set()
    try:
        await page.goto(category_url, timeout=GOTO_TIMEOUT_CATEGORY, wait_until="domcontentloaded")
        await page.wait_for_timeout(SHORT_WAIT)
    except Exception as e:
        print(f"> Timeout/Error en goto category {category_url}: {e}")
        return news_links

    for i in range(max_clicks):
        boton = await page.query_selector(load_more_selector)
        if boton is None:
            break
        try:
            await page.evaluate("(btn) => btn.scrollIntoView()", boton)
            await boton.click(force=True)
            await page.wait_for_timeout(CLICK_WAIT)
        except Exception:
            break

    soup = BeautifulSoup(await page.content(), "html.parser")
    for a_tag in soup.find_all("a", href=True):
        link = a_tag["href"]
        if link.startswith("/"):
            link = urljoin(category_url, link)
        if any(p in link for p in news_pattern):
            news_links.add(link)

    return news_links

# Crawl de las categorias de noticias de la p√°gina
async def crawl_categories(site_config):
    start_url = site_config["start_url"]
    category_pattern = site_config["category_pattern"]

    category_links = set()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await _block_assets(page)
        try:
            await page.goto(start_url, timeout=GOTO_TIMEOUT_START, wait_until="domcontentloaded")
            await page.wait_for_timeout(SHORT_WAIT)
        except Exception as e:
            print(f"> Timeout/Err en goto start_url (categories) {start_url}: {e}")
            await browser.close()
            return set()

        # Obtiene categorias
        soup = BeautifulSoup(await page.content(), "html.parser")
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            if href.startswith("/"):
                href = urljoin(start_url, href)
            if category_pattern in href:
                category_links.add(href)

        await browser.close()
            
    return category_links


# Crawl de los links de noticias de cada categor√≠a
async def crawl_news(site_config, category_links):
    start_url = site_config["start_url"]
    news_pattern = site_config["news_pattern"]
    pagination_type = site_config["pagination_type"]

    news = set()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await _block_assets(page)
        try:
            await page.goto(start_url, timeout=GOTO_TIMEOUT_START, wait_until="domcontentloaded")
            await page.wait_for_timeout(SHORT_WAIT)
        except Exception as e:
            print(f"> Timeout/Err en goto start_url (crawl_news) {start_url}: {e}")
            await browser.close()
            return set()

        # Inicia scrap categorias
        for cat_url in category_links:
            print(f"> {start_url} ‚Üí {cat_url}")
            if pagination_type == "loadmore":
                cat_news = await scrape_category_loadmore(
                    page,
                    cat_url,
                    site_config["load_more_selector"],
                    news_pattern,
                    site_config["max_clicks"]
                )
            else:
                cat_news = set()


            
            slug = cat_url.rstrip("/").split("/")[-1]

            for link in cat_news:
                try:
                    categoria = slug  # valor por defecto

                    # extrae la parte relevante despu√©s de /especial/ o /noticias/
                    if "/especial/" in link:
                        path = link.split("/especial/", 1)[1]
                    elif "/noticias/" in link:
                        path = link.split("/noticias/", 1)[1]
                    else:
                        path = None

                    if path:
                        parts = [p for p in path.split("/") if p]  # filtra vac√≠os
                        categorias_parts = []
                        for part in parts:
                            # detener si encontramos un a√±o (4 d√≠gitos)
                            if part.isdigit() and len(part) == 4:
                                break
                            categorias_parts.append(part)
                            if len(categorias_parts) == 3:  # m√°ximo 3 niveles
                                break

                        # si el primer segmento es "biobiochile" lo descartamos
                        if categorias_parts and categorias_parts[0].lower() == "biobiochile":
                            categorias_parts = categorias_parts[1:]

                        # quitar segmento redundante "noticias" (ej. bbcl-investiga/noticias/articulos -> bbcl-investiga/articulos)
                        if len(categorias_parts) >= 2 and categorias_parts[1].lower() == "noticias":
                            categorias_parts.pop(1)

                        # caso especial: noticias-patrocinadas -> usar solo 'noticias-patrocinadas'
                        if categorias_parts and categorias_parts[0].lower() == "noticias-patrocinadas":
                            categoria = "noticias-patrocinadas"
                        elif categorias_parts:
                            categoria = "/".join(categorias_parts)
                        else:
                            categoria = slug
                except Exception:
                    categoria = slug  # fallback
                news.add((categoria, link))

        await browser.close()
    
    return news


async def main():
    for site, config in SITES.items():
        print(f"\nüåê CRAWLEANDO SITIO: {site}")

        start_time = time.time() #inicio medicion tiempo

        categorias = await crawl_categories(config)

        print(f">> Total categorias encontradas en {site}: {len(categorias)}\n")

        total_categorias = len(categorias)

        all_news = set()
        tope = 0

        categorias = list(categorias)

        # Avance procesado en lotes para futura paralelizaci√≥n
        for i in range (0, len(categorias), 10):
            if i + 10 < len(categorias):
                tope = i+10
            else:
                tope = len(categorias)
            news_batch = await crawl_news(config, categorias[i:tope])
            all_news.update(news_batch)
        
        # Metricas del crawler
        duracion = time.time() - start_time
        total_urls = len(all_news)
        promedio_por_categoria = total_urls/total_categorias if total_categorias > 0 else 0
        urls_por_minuto = total_urls / (duracion / 60) if duracion > 0 else 0

        os.makedirs("metrics", exist_ok=True)

        with open("metrics/crawler_metrics.json", "w", encoding="utf-8") as f:
            json.dump({
                "sitio": site,
                "total_categorias": total_categorias,
                "total_urls_encontradas": total_urls,
                "urls_por_categoria": round(promedio_por_categoria, 3),
                "duracion_segundos": round(duracion, 2),
                "urls_por_minuto": round(urls_por_minuto, 2)
            }, f, ensure_ascii=False, indent=4)



        # Guardar links en un csv
        csv_file_path = f"Crawler/{site}.csv"
        with open(csv_file_path, 'w', newline='') as csvfile:
            csv_writer = csv.writer(csvfile)
            for categoria, link in list(all_news):
                csv_writer.writerow([categoria, link])
        
        print(f"Links guardados en {csv_file_path}")


# Ejecutar
if __name__ == "__main__":
    asyncio.run(main())